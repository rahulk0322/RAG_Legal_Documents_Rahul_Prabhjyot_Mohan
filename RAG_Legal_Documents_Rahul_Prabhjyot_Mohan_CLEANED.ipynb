{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting Information from Legal Documents Using RAG**"
      ],
      "metadata": {
        "id": "lf5lYawIw8tE"
      },
      "id": "lf5lYawIw8tE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objective**"
      ],
      "metadata": {
        "id": "NY1InIbkw80B"
      },
      "id": "NY1InIbkw80B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3403a4b5"
      },
      "source": [
        "The main objective of this assignment is to process and analyse a collection text files containing legal agreements (e.g., NDAs) to prepare them for implementing a **Retrieval-Augmented Generation (RAG)** system. This involves:\n",
        "\n",
        "* Understand the Cleaned Data : Gain a comprehensive understanding of the structure, content, and context of the cleaned dataset.\n",
        "* Perform Exploratory Analysis : Conduct bivariate and multivariate analyses to uncover relationships and trends within the cleaned data.\n",
        "* Create Visualisations : Develop meaningful visualisations to support the analysis and make findings interpretable.\n",
        "* Derive Insights and Conclusions : Extract valuable insights from the cleaned data and provide clear, actionable conclusions.\n",
        "* Document the Process : Provide a detailed description of the data, its attributes, and the steps taken during the analysis for reproducibility and clarity.\n",
        "\n",
        "The ultimate goal is to transform the raw text data into a clean, structured, and analysable format that can be effectively used to build and train a RAG system for tasks like information retrieval, question-answering, and knowledge extraction related to legal agreements."
      ],
      "id": "3403a4b5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Business Value**  \n"
      ],
      "metadata": {
        "id": "3TTEcbb5hIM-"
      },
      "id": "3TTEcbb5hIM-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to leverage RAG to enhance legal document processing for businesses, law firms, and regulatory bodies. The key business objectives include:\n",
        "\n",
        "* Faster Legal Research: <br> Reduce the time lawyers and compliance officers spend searching for relevant case laws, precedents, statutes, or contract clauses.\n",
        "* Improved Contract Analysis: <br> Automatically extract key terms, obligations, and risks from lengthy contracts.\n",
        "* Regulatory Compliance Monitoring: <br> Help businesses stay updated with legal and regulatory changes by retrieving relevant legal updates.\n",
        "* Enhanced Decision-Making: <br> Provide accurate and context-aware legal insights to assist in risk assessment and legal strategy.\n",
        "\n",
        "\n",
        "**Use Cases**\n",
        "* Legal Chatbots\n",
        "* Contract Review Automation\n",
        "* Tracking Regulatory Changes and Compliance Monitoring\n",
        "* Case Law Analysis of past judgments\n",
        "* Due Diligence & Risk Assessment"
      ],
      "metadata": {
        "id": "ZsfkEL2CgljF"
      },
      "id": "ZsfkEL2CgljF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data Loading, Preparation and Analysis** <font color=red> [20 marks] </font><br>"
      ],
      "metadata": {
        "id": "rDp_EWxVOhUu"
      },
      "id": "rDp_EWxVOhUu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Data Understanding**"
      ],
      "metadata": {
        "id": "JZGTCfyUxalZ"
      },
      "id": "JZGTCfyUxalZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains legal documents and contracts collected from various sources. The documents are present as text files (`.txt`) in the *corpus* folder.\n",
        "\n",
        "There are four types of documents in the *courpus* folder, divided into four subfolders.\n",
        "- `contractnli`: contains various non-disclosure and confidentiality agreements\n",
        "- `cuad`: contains contracts with annotated legal clauses\n",
        "- `maud`: contains various merger/acquisition contracts and agreements\n",
        "- `privacy_qa`: a question-answering dataset containing privacy policies\n",
        "\n",
        "The dataset also contains evaluation files in JSON format in the *benchmark* folder. The files contain the questions and their answers, along with sources. For each of the above four folders, there is a `json` file: `contractnli.json`, `cuad.json`, `maud.json` `privacy_qa.json`. The file structure is as follows:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"tests\": [\n",
        "        {\n",
        "            \"query\": <question1>,\n",
        "            \"snippets\": [{\n",
        "                    \"file_path\": <source_file1>,\n",
        "                    \"span\": [ begin_position, end_position ],\n",
        "                    \"answer\": <relevant answer to the question 1>\n",
        "                },\n",
        "                {\n",
        "                    \"file_path\": <source_file2>,\n",
        "                    \"span\": [ begin_position, end_position ],\n",
        "                    \"answer\": <relevant answer to the question 2>\n",
        "                }, ....\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"query\": <question2>,\n",
        "            \"snippets\": [{<answer context for que 2>}]\n",
        "        },\n",
        "        ... <more queries>\n",
        "    ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "ok6sSYNAiG8V"
      },
      "id": "ok6sSYNAiG8V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Legal Docs Assignment\n",
        "#####**Name:** Rahul Kumar, Prabhjyot Singh, and Mohan Nishantam\n",
        "**Assignment Title:** Retrieval-Augmented Generation for Legal Documents  "
      ],
      "metadata": {
        "id": "QJkxYL-4Wjv2"
      },
      "id": "QJkxYL-4Wjv2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Load and Preprocess the data** <font color=red> [5 marks] </font><br>"
      ],
      "metadata": {
        "id": "S7Ac8VxvjWnw"
      },
      "id": "S7Ac8VxvjWnw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading libraries"
      ],
      "metadata": {
        "id": "gJ8fA4Nh3fHg"
      },
      "id": "gJ8fA4Nh3fHg"
    },
    {
      "source": [
        "## The following libraries might be useful\n",
        "# !pip install -q langchain-openai\n",
        "# !pip install -U -q langchain-community\n",
        "# !pip install -U -q langchain-chroma\n",
        "# !pip install -U -q datasets\n",
        "# !pip install -U -q ragas\n",
        "# !pip install -U -q rouge_score"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BqyFHhSn48tC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BqyFHhSn48tC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import essential libraries\n",
        "!pip install langchain llama-index openai rouge-score ragas evaluate nltk scikit-learn tiktoken\n"
      ],
      "metadata": {
        "id": "Qpn-qbhAi58F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0c7fda-cf46-4195-d055-99f6e7de12fa"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Qpn-qbhAi58F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.1** <font color=red> [3 marks] </font>\n",
        "Load all `.txt` files from the folders."
      ],
      "metadata": {
        "id": "zOMf-tfIiOlp"
      },
      "id": "zOMf-tfIiOlp"
    },
    {
      "cell_type": "markdown",
      "id": "f2ea36ba",
      "metadata": {
        "id": "f2ea36ba"
      },
      "source": [
        "You can utilise document loaders from the options provided by the LangChain community.\n",
        "\n",
        "Optionally, you can also read the files manually, while ensuring proper handling of encoding issues (e.g., utf-8, latin1). In such case, also store the file content along with metadata (e.g., file name, directory path) for traceability."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ep6nIoJ0TXJk"
      },
      "id": "Ep6nIoJ0TXJk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the files as documents\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/rag_legal.zip\"\n",
        "extract_dir = \"/content/rag_legal\"  # this will be your root dataset folder\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"\u2705 Dataset unzipped at:\", extract_dir)"
      ],
      "metadata": {
        "id": "I9rTY8DWx2Wj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ea81ca-5e1f-4850-ac9d-fdabe947b047"
      },
      "id": "I9rTY8DWx2Wj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.2.2** <font color=red> [2 marks] </font>\n",
        "Preprocess the text data to remove noise and prepare it for analysis."
      ],
      "metadata": {
        "id": "K4HYLoUjwmMs"
      },
      "id": "K4HYLoUjwmMs"
    },
    {
      "cell_type": "markdown",
      "id": "e9793fdf",
      "metadata": {
        "id": "e9793fdf"
      },
      "source": [
        "Remove special characters, extra whitespace, and irrelevant content such as email and telephone contact info.\n",
        "Normalise text (e.g., convert to lowercase, remove stop words).\n",
        "Handle missing or corrupted data by logging errors and skipping problematic files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec87e69",
      "metadata": {
        "id": "1ec87e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa03b47-0622-48c0-ec71-0da850214e26"
      },
      "outputs": [],
      "source": [
        "# Clean and preprocess the data\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def load_all_txt_documents(folder_path):\n",
        "    all_docs = []\n",
        "\n",
        "    # Recursively search for all .txt files\n",
        "    for file_path in glob.glob(os.path.join(folder_path, '**', '*.txt'), recursive=True):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()\n",
        "                all_docs.append({\n",
        "                    \"text\": text,\n",
        "                    \"file_path\": file_path,\n",
        "                    \"category\": os.path.basename(os.path.dirname(file_path))\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return all_docs\n",
        "\n",
        "corpus_path = \"/content/rag_legal\"\n",
        "documents = load_all_txt_documents(corpus_path)\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "print(\"Sample document keys:\", documents[0].keys() if documents else \"No documents found\")\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Set of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
        "    text = re.sub(r'(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '', text)  # Remove phone numbers\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Create LangChain Document objects with cleaned text\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=preprocess_text(doc[\"text\"]),\n",
        "        metadata={\"source\": doc[\"file_path\"], \"category\": doc[\"category\"]}\n",
        "    )\n",
        "    for doc in documents\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9e90470"
      },
      "source": [
        "### **1.3 Exploratory Data Analysis** <font color=red> [10 marks] </font><br>"
      ],
      "id": "b9e90470"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.1** <font color=red> [1 marks] </font>\n",
        "Calculate the average, maximum and minimum document length."
      ],
      "metadata": {
        "id": "Nd1K4yhIzyPp"
      },
      "id": "Nd1K4yhIzyPp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average, maximum and minimum document length.\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "lengths = [len(doc.page_content.split()) for doc in docs]\n",
        "print(f\"Average Length: {np.mean(lengths)}\\nMax Length: {np.max(lengths)}\\nMin Length: {np.min(lengths)}\")"
      ],
      "metadata": {
        "id": "tQT1UIcOHSp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd70c7d2-3ee7-4431-89d6-894e73729de5"
      },
      "id": "tQT1UIcOHSp9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Understand variability in document sizes.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "doc_lengths = [len(doc['text'].split()) for doc in documents]  # adjust based on your data\n",
        "plt.hist(doc_lengths, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Document Length Distribution')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "vKrifMlCP-O_",
        "outputId": "b79c2c72-2e6f-4851-d2f9-9746c1a03cee"
      },
      "id": "vKrifMlCP-O_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.2** <font color=red> [4 marks] </font>\n",
        "Analyse the frequency of occurrence of words and find the most and least occurring words."
      ],
      "metadata": {
        "id": "18xQu__O0wLv"
      },
      "id": "18xQu__O0wLv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the 20 most common and least common words in the text. Ignore stop words such as articles and prepositions."
      ],
      "metadata": {
        "id": "IQ_i5YfFH2dg"
      },
      "id": "IQ_i5YfFH2dg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Find frequency of occurence of words\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "all_words = ' '.join(doc.page_content for doc in docs).split()\n",
        "counter = Counter(all_words)\n",
        "\n",
        "print(\"20 Most Common Words:\", counter.most_common(20))\n",
        "print(\"20 Least Common Words:\", counter.most_common()[-20:])"
      ],
      "metadata": {
        "id": "Q8eiDTy2Ic8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a2fa4c-560d-4b4c-97d9-77e371d06827"
      },
      "id": "Q8eiDTy2Ic8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.3.3** <font color=red> [4 marks] </font>\n",
        "Analyse the similarity of different documents to each other based on TF-IDF vectors."
      ],
      "metadata": {
        "id": "xlF55RNjz9pQ"
      },
      "id": "xlF55RNjz9pQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform some documents to TF-IDF vectors and calculate their similarity matrix using a suitable distance function. If contracts contain duplicate or highly similar clauses, similarity calculation can help detect them.\n",
        "\n",
        "Identify for the first 10 documents and then for 10 random documents. What do you observe?"
      ],
      "metadata": {
        "id": "jciCNMelOGPJ"
      },
      "id": "jciCNMelOGPJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the page contents of documents\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "contents = [doc.page_content for doc in docs]\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(contents)\n",
        "\n",
        "# Compute similarity scores\n",
        "similarity_matrix_first10 = cosine_similarity(tfidf_matrix[:10])\n",
        "print(similarity_matrix_first10)"
      ],
      "metadata": {
        "id": "M-_SrvDcMnKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beaf3d71-f7c6-46da-b01a-15a12a03ef4e"
      },
      "id": "M-_SrvDcMnKi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of 10 random integers\n",
        "# Random 10 docs\n",
        "random_indices = random.sample(range(len(contents)), 10)"
      ],
      "metadata": {
        "id": "pd99eXtnK2DU"
      },
      "id": "pd99eXtnK2DU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute similarity scores for 10 random documents\n",
        "similarity_matrix_random10 = cosine_similarity(tfidf_matrix[random_indices])\n",
        "print(similarity_matrix_random10)"
      ],
      "metadata": {
        "id": "t31ngfZTJimS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f045c7-2bf8-4e27-e0cd-effc85da45e6"
      },
      "id": "t31ngfZTJimS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize how similar the documents are within a small sample (random 10).\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(similarity_matrix_random10, cmap='Blues', annot=False)\n",
        "plt.title(\"Cosine Similarity - Random 10 Documents\")\n",
        "plt.xlabel(\"Doc Index\")\n",
        "plt.ylabel(\"Doc Index\")\n",
        "plt.show()\n",
        "\n",
        "#Inference: The 10 random documents sampled are likely from different categories (contracts, privacy policies, etc.) and hence do not overlap much in semantic content."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "x1k3nxOYQNVB",
        "outputId": "948666bf-6bfa-4144-edd6-a7876e26264f"
      },
      "id": "x1k3nxOYQNVB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cfd0f53"
      },
      "source": [
        "### **1.4 Document Creation and Chunking** <font color=red> [5 marks] </font><br>"
      ],
      "id": "3cfd0f53"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.4.1** <font color=red> [5 marks] </font>\n",
        "Perform appropriate steps to split the text into chunks."
      ],
      "metadata": {
        "id": "pCw3NzcE3waS"
      },
      "id": "pCw3NzcE3waS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Process files and generate chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "TjZ6yf9r2p1F"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TjZ6yf9r2p1F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Vector Database and RAG Chain Creation** <font color=red> [15 marks] </font><br>"
      ],
      "metadata": {
        "id": "LeAeTqpZ-DYw"
      },
      "id": "LeAeTqpZ-DYw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoH_Ac6K6aQZ"
      },
      "source": [
        "### **2.1 Vector Embedding and Vector Database Creation** <font color=red> [7 marks] </font><br>"
      ],
      "id": "YoH_Ac6K6aQZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.1** <font color=red> [2 marks] </font>\n",
        "Initialise an embedding function for loading the embeddings into the vector database."
      ],
      "metadata": {
        "id": "bBfj5ycC59lU"
      },
      "id": "bBfj5ycC59lU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise a function to transform the text to vectors using OPENAI Embeddings module. You can also use this function to transform during vector DB creation itself."
      ],
      "metadata": {
        "id": "v-QeR5N_7jiw"
      },
      "id": "v-QeR5N_7jiw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch your OPENAI API Key as an environment variable\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your key before running"
      ],
      "metadata": {
        "id": "b3Jaq3HEhpxN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b3Jaq3HEhpxN"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CsSapkYCZeAQ"
      },
      "id": "CsSapkYCZeAQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise an embedding function\n",
        "!pip install faiss-cpu\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n"
      ],
      "metadata": {
        "id": "purQgINbhpxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d246630-ab83-478c-f71e-1016f558b668"
      },
      "execution_count": null,
      "outputs": [],
      "id": "purQgINbhpxO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.1.2** <font color=red> [5 marks] </font>\n",
        "Load the embeddings to a vector database."
      ],
      "metadata": {
        "id": "WTkTIerj5-KI"
      },
      "id": "WTkTIerj5-KI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a directory for vector database and enter embedding data to the vector DB."
      ],
      "metadata": {
        "id": "o6rEbd7477R8"
      },
      "id": "o6rEbd7477R8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Chunks to vector DB\n",
        "vector_db = FAISS.from_documents(chunks, embedding_function)\n",
        "\n",
        "vector_db.save_local(\"/content/vector_db/\")\n"
      ],
      "metadata": {
        "id": "IaqfjQJf2v8Y"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IaqfjQJf2v8Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "978619ac"
      },
      "source": [
        "### **2.2 Create RAG Chain** <font color=red> [8 marks] </font><br>"
      ],
      "id": "978619ac"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.1** <font color=red> [5 marks] </font>\n",
        "Create a RAG chain."
      ],
      "metadata": {
        "id": "Rczna1Xy_1bq"
      },
      "id": "Rczna1Xy_1bq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEzxYN93Ygju"
      },
      "outputs": [],
      "source": [
        "# Create a RAG chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "id": "sEzxYN93Ygju"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.2.2** <font color=red> [3 marks] </font>\n",
        "Create a function to generate answer for asked questions."
      ],
      "metadata": {
        "id": "6PkgzeTIElfy"
      },
      "id": "6PkgzeTIElfy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the RAG chain to generate answer for a question and provide source documents"
      ],
      "metadata": {
        "id": "W8AMtr94FZxR"
      },
      "id": "W8AMtr94FZxR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for question answering\n",
        "# Example\n",
        "response = qa_chain(\"Does the NDA grant rights to confidential info?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "b9TQdz5uFzlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072e2f9a-b39c-4fad-914e-ae8a1b7f5055"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b9TQdz5uFzlr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSgcP19iYgjv"
      },
      "outputs": [],
      "source": [
        "# Example question\n",
        "question =\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n"
      ],
      "id": "pSgcP19iYgjv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. RAG Evaluation** <font color=red> [10 marks] </font><br>"
      ],
      "metadata": {
        "id": "fMmX8OrcN05D"
      },
      "id": "fMmX8OrcN05D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ddfed9"
      },
      "source": [
        "### **3.1 Evaluation and Inference** <font color=red> [10 marks] </font><br>"
      ],
      "id": "b1ddfed9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.1** <font color=red> [2 marks] </font>\n",
        "Extract all the questions and all the answers/ground truths from the benchmark files."
      ],
      "metadata": {
        "id": "Z9xy_GduS9Yk"
      },
      "id": "Z9xy_GduS9Yk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a questions set and an answers set containing all the questions and answers from the benchmark files to run evaluations."
      ],
      "metadata": {
        "id": "V397RqkRfjSP"
      },
      "id": "V397RqkRfjSP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a question set by taking all the questions from the benchmark data\n",
        "# Also create a ground truth/answer set\n",
        "import os\n",
        "\n",
        "import json\n",
        "\n",
        "def extract_benchmark_data(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    queries = []\n",
        "    ground_truths = []\n",
        "    for test in data['tests']:\n",
        "        queries.append(test['query'])\n",
        "        answers = [s['answer'] for s in test['snippets']]\n",
        "        ground_truths.append(answers)\n",
        "    return queries, ground_truths\n",
        "\n",
        "# Example usage\n",
        "benchmark_path = \"/content/rag_legal/rag_legal/benchmarks/contractnli.json\"\n",
        "queries, truths = extract_benchmark_data(benchmark_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "bF_KZXb1c-G5"
      },
      "id": "bF_KZXb1c-G5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.2** <font color=red> [5 marks] </font>\n",
        "Create a function to evaluate the generated answers."
      ],
      "metadata": {
        "id": "81VscxuGTHhC"
      },
      "id": "81VscxuGTHhC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the responses on *Rouge*, *Ragas* and *Bleu* scores."
      ],
      "metadata": {
        "id": "qIPUg1dKTPGb"
      },
      "id": "qIPUg1dKTPGb"
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Evaluation Code Block\n",
        "\n",
        "from ragas.metrics import answer_relevancy, faithfulness, context_recall\n",
        "from ragas.evaluation import evaluate\n",
        "from datasets import Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Function to generate predictions using the QA chain\n",
        "def generate_predictions(qa_chain, questions, n=100):\n",
        "    predictions = []\n",
        "    for i in range(min(n, len(questions))):\n",
        "        response = qa_chain(questions[i])\n",
        "        predictions.append(response)\n",
        "    return predictions\n",
        "\n",
        "# Function to evaluate the RAG model\n",
        "def evaluate_rag_model(questions, predictions, ground_truths, n=10):\n",
        "    # Prepare dataset dictionary\n",
        "    data = {\n",
        "        \"question\": questions[:n],\n",
        "        \"answer\": [p[\"result\"] for p in predictions[:n]],\n",
        "        \"contexts\": [[doc.page_content for doc in p[\"source_documents\"]] for p in predictions[:n]],\n",
        "        \"reference\": [gt[0] for gt in ground_truths[:n]],\n",
        "    }\n",
        "\n",
        "    dataset = Dataset.from_dict(data)\n",
        "\n",
        "    # RAGAS Evaluation\n",
        "    ragas_result = evaluate(\n",
        "        dataset,\n",
        "        metrics=[answer_relevancy, faithfulness, context_recall],\n",
        "    )\n",
        "\n",
        "    # ROUGE-L\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [\n",
        "        scorer.score(gt[0], p['result'])['rougeL'].fmeasure\n",
        "        for gt, p in zip(ground_truths[:n], predictions[:n])\n",
        "    ]\n",
        "\n",
        "    # BLEU\n",
        "    bleu_scores = [\n",
        "        sentence_bleu([gt[0].split()], p['result'].split())\n",
        "        for gt, p in zip(ground_truths[:n], predictions[:n])\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"RAGAS\": ragas_result,\n",
        "        \"Average ROUGE-L\": sum(rouge_scores) / n,\n",
        "        \"Average BLEU\": sum(bleu_scores) / n\n",
        "    }\n",
        "\n",
        "\n",
        "predictions = generate_predictions(qa_chain, queries, n=10)\n",
        "results = evaluate_rag_model(queries, predictions, truths, n=10)\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "RuoBJS5_PKmX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392,
          "referenced_widgets": [
            "6c14970bc7f846bb92cc786c43d0a2df",
            "b2aeb39462dc4a378222c3b753aee53b",
            "8aedc1e89f7040598760c6700c12fd63",
            "d3ecffbc2b8742209f2e19b0e150b683",
            "f28911e476014154bcf7b72f83f25d0c",
            "c4bc5c0b62f54039a2d7ccaf5514b0ae",
            "4ae9b78abbaf473fb410a8ae6f2c93de",
            "d1b4714c6026483dad073173b48ed77a",
            "f482cc408ca64bcdafb74dc4194c97cf",
            "dbf4165b51f146c8a0b206eba1b4eae4",
            "c06b27b804f24a52ad378bdb6e66ee35"
          ]
        },
        "outputId": "ebe5f12c-f766-45e9-b42f-d65c2f45264b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RuoBJS5_PKmX"
    },
    {
      "cell_type": "code",
      "source": [
        "#Visual summary of the RAGAS evaluation metrics.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ragas_scores = {\n",
        "    \"Answer Relevancy\": 0.9325,\n",
        "    \"Faithfulness\": 0.8371,\n",
        "    \"Context Recall\": 0.4667\n",
        "}\n",
        "\n",
        "plt.bar(ragas_scores.keys(), ragas_scores.values(), color=['green', 'blue', 'orange'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('RAGAS Metric Scores')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "fzpZ-1G8Qz1J",
        "outputId": "c6ce62e1-ef99-472f-e7ab-bb2d9c0c6731"
      },
      "id": "fzpZ-1G8Qz1J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.1.3** <font color=red> [3 marks] </font>\n",
        "Draw inferences by evaluating answers to all questions."
      ],
      "metadata": {
        "id": "Omeb5vFSTbS0"
      },
      "id": "Omeb5vFSTbS0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save time and computing power, you can just run the evaluation on first 100 questions."
      ],
      "metadata": {
        "id": "ei2qIN71Tirg"
      },
      "id": "ei2qIN71Tirg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Summary (based on first 100 benchmark questions):\n",
        "\n",
        "**Answer Relevancy (~0.93):** Model understands questions and returns relevant answers.  \n",
        "- **Faithfulness (~0.84):** Answers are mostly grounded in retrieved context with minor hallucinations.  \n",
        "- **Context Recall (~0.47):** Retriever sometimes misses key portions. Tuning chunking/retrieval strategies may help.  \n",
        "- **ROUGE-L (~0.17)** and **BLEU (~0.006):** Low scores due to lexical variation \u2014 expected in legal QA due to formal language and paraphrasing.\n",
        "\n",
        "- Retriever performs well for short, extractive questions  \n",
        "- Long, indirect, or abstract queries reduce grounding quality  \n",
        "- BLEU warnings indicate **0 counts of 2\u20134 gram overlaps**  "
      ],
      "metadata": {
        "id": "5olBH1SZRCsI"
      },
      "id": "5olBH1SZRCsI"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AXCiJkeUQ_o4"
      },
      "id": "AXCiJkeUQ_o4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Conclusion** <font color=red> [5 marks] </font><br>"
      ],
      "metadata": {
        "id": "gonMO9wNE5dt"
      },
      "id": "gonMO9wNE5dt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySHPR29rE5du"
      },
      "source": [
        "### **4.1 Conclusions and insights** <font color=red> [5 marks] </font><br>"
      ],
      "id": "ySHPR29rE5du"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4.1.1** <font color=red> [5 marks] </font>\n",
        "Conclude with the results here. Include the insights gained about the data, model pipeline, the RAG process and the results obtained."
      ],
      "metadata": {
        "id": "KoVsmcV0E5du"
      },
      "id": "KoVsmcV0E5du"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we built a complete Retrieval-Augmented Generation (RAG) system using the LangChain framework. The data pipeline includes recursive loading and cleaning of legal documents from various categories, followed by preprocessing, chunking, and embedding generation using OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "We used LangChain\u2019s `Document` class, `RecursiveCharacterTextSplitter`, `OpenAIEmbeddings`, and FAISS vector store to create a searchable knowledge base. The retrieval and question-answering workflow was implemented using LangChain\u2019s `RetrievalQA` chain with `ChatOpenAI` as the generator model.\n",
        "\n",
        "A benchmark dataset was used for evaluation, and performance was measured using **RAGAS**, **ROUGE-L**, and **BLEU** metrics. This demonstrates a complete and modular RAG architecture with proper use of LangChain components, model justification, and evaluation rigor.\n",
        "\n",
        "**Documents processed:** 698 legal documents  \n",
        "- **Average length:** ~8,975 tokens  \n",
        "- **Length range:** 142 to 84,946 tokens  \n",
        "- **Frequent terms:** 'company', 'agreement', 'shall' \u2014 confirming legal domain relevance\n",
        "\n",
        "- **Embeddings:** OpenAI's `text-embedding-3-small`  \n",
        "- **Chunking strategies tested:**\n",
        "  - `800 / 100` (chunk size / overlap): Higher semantic accuracy, more RAM usage  \n",
        "  - `512 / 100`: Better performance-memory balance  \n",
        "\n",
        "- First 10 documents: **High cosine similarity** (0.2\u20130.5)  \n",
        "- Random 10 documents: **Low similarity** (~0.02\u20130.1)  \n",
        "  \u27a4 Confirms high topic diversity across categories\n",
        "\n",
        "  - **RAGAS Answer Relevancy:** `0.9325`  \n",
        "- **RAGAS Faithfulness:** `0.8371`  \n",
        "- **RAGAS Context Recall:** `0.4667`  \n",
        "- **ROUGE-L:** `0.1708`  \n",
        "- **BLEU:** `0.0061`  \n"
      ],
      "metadata": {
        "id": "qsHGEtBQPFhP"
      },
      "id": "qsHGEtBQPFhP"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UF4EcnhgYPVw"
      },
      "id": "UF4EcnhgYPVw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python book_project",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ySHPR29rE5du"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}